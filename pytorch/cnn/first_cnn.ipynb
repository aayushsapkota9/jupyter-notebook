{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFR2qp06AUI3gqWBgIrmYP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayushsapkota9/jupyter-notebook/blob/main/pytorch/cnn/first_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yGxwZvlWrGkj"
      },
      "outputs": [],
      "source": [
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n"
      ],
      "metadata": {
        "id": "73srYPqirdfr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "tZtPki5MreHy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use transforms.compose method to reformat images for modeling,\n",
        "# and save to variable all_transforms for later use\n",
        "all_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                                                          std=[0.2023, 0.1994, 0.2010])\n",
        "                                     ])\n",
        "# Create Training dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                             train = True,\n",
        "                                             transform = all_transforms,\n",
        "                                             download = True)\n",
        "\n",
        "# Create Testing dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                            train = False,\n",
        "                                            transform = all_transforms,\n",
        "                                            download=True)\n",
        "\n",
        "# Instantiate loader objects to facilitate processing\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ],
      "metadata": {
        "id": "eiSFAhuarZgx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fc46c3-c59b-46d2-d8e8-8b02cef15b28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 35.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a CNN class\n",
        "class ConvNeuralNet(nn.Module):\n",
        "#  Determine what layers and their order in CNN object\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(1600, 128)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    # Progresses data across layers\n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.max_pool1(out)\n",
        "\n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "DPl3aUuc8hLZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNeuralNet(num_classes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Set Loss function with criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set optimizer with optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)\n",
        "\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "VsSQKhNuPuyI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "# Load in the data in batches using the train_loader object\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5ccJsA-RGN4",
        "outputId": "17894ba3-df04-4e67-e031-9ba8843e4331"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.8093\n",
            "Epoch [2/20], Loss: 1.7195\n",
            "Epoch [3/20], Loss: 1.2912\n",
            "Epoch [4/20], Loss: 1.4396\n",
            "Epoch [5/20], Loss: 1.7280\n",
            "Epoch [6/20], Loss: 1.2673\n",
            "Epoch [7/20], Loss: 0.9312\n",
            "Epoch [8/20], Loss: 1.2343\n",
            "Epoch [9/20], Loss: 1.2735\n",
            "Epoch [10/20], Loss: 1.2000\n",
            "Epoch [11/20], Loss: 1.0557\n",
            "Epoch [12/20], Loss: 0.8952\n",
            "Epoch [13/20], Loss: 0.9886\n",
            "Epoch [14/20], Loss: 0.5955\n",
            "Epoch [15/20], Loss: 0.8100\n",
            "Epoch [16/20], Loss: 0.8750\n",
            "Epoch [17/20], Loss: 0.7304\n",
            "Epoch [18/20], Loss: 0.3569\n",
            "Epoch [19/20], Loss: 0.5590\n",
            "Epoch [20/20], Loss: 0.6341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ASURYRUrPA",
        "outputId": "d55e5d64-3c7b-4f8c-fa71-9156e6bc149f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50000 train images: 80.88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "WFcLlELPXNp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model weights to a file\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path for the directory\n",
        "model_dir = \"/content/drive/MyDrive/models\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "torch.save(\n",
        "    model.state_dict(),\n",
        "    os.path.join(model_dir, \"mnist_model.pth\")\n",
        ")"
      ],
      "metadata": {
        "id": "JVKFL988XGsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f87e44-3672-41ef-a74b-3f5d49c8137c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reusing the model"
      ],
      "metadata": {
        "id": "Vyn8f2VZXLpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Re-create the hollow structure\n",
        "model = ConvNeuralNet(num_classes)\n",
        "\n",
        "# 2. Load the weights into that structure\n",
        "model.load_state_dict(\n",
        "    torch.load(\"/content/drive/MyDrive/models/mnist_model.pth\", map_location=device)\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "# 3. Set to evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "Stvb3tihXIuC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0b16bac-b602-44c6-a3cf-b08333293b3d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNeuralNet(\n",
              "  (conv_layer1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv_layer2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv_layer3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv_layer4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=1600, out_features=128, bias=True)\n",
              "  (relu1): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the CIFAR-10 class names\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Get a random image from the test set\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Move images and labels to the appropriate device\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# Make a prediction\n",
        "outputs = model(images)\n",
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Display the first image\n",
        "img = images[0].cpu().numpy()\n",
        "label = labels[0].item()\n",
        "predicted_label = predicted[0].item()\n",
        "\n",
        "# Unnormalize the image for display\n",
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std = np.array([0.2023, 0.1994, 0.2010])\n",
        "img = std.reshape(-1, 1, 1) * img + mean.reshape(-1, 1, 1)\n",
        "img = np.clip(img, 0, 1)\n",
        "\n",
        "plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "plt.title(f\"True: {classes[label]}, Predicted: {classes[predicted_label]}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "SBl6G5csv8aY",
        "outputId": "536fe495-d6ac-410d-e871-287f170113f9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJwJJREFUeJzt3XuQXXWZ7vFn7ful9+7uJJ0L3ZCEBJgMGq3JAaosYpg0t5SoIIzOhQrEwRs6hVDoCDOQBI7OoM4YSoqDihNg6pTcHBimVGJqDBOHcSJ6DhCVWyQhkHRIupO+d+/rOn948iuaTuj31YSgfj9V/JHmzZvVa6/dT+/uXk9HcRzHAgBAUuJYHwAA4K2DUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgIBQwK/trrvuUhRF+slPfjLl7FlnnaWzzjrr6B/UMbZmzRpFUTThbfPmzdPll19+bA7oEA51jEfSY489piiK9OCDD045e/nll2vevHlH7VjgRygcRVEUmf577LHHjvWh/s547XlNJBI67rjjdO655/7WnePdu3drzZo1evLJJ4/1oeD3TOpYH8Dvsn/+53+e8Od77rlHGzdunPT2RYsWvZmHdUx8//vff9P+rXPOOUcrV65UHMfavn27br/9di1fvlzf+c53tGLFijftOA567rnnlEj4Pv/avXu31q5dq3nz5umd73zn0Tmwt4BvfOMbajabx/ow8BqEwlF06aWXTvjzf//3f2vjxo2T3v56o6OjKhQKR/PQ3nSZTOZN+7dOPvnkCef4oosu0uLFi7Vu3brDhsL4+LgymYz7g7dFNps94jt/V6TT6WN9CHgdvnx0jJ111ll629vepp/+9Kd697vfrUKhoOuvv17Sr74UsmbNmkl/51Bfo+7v79enP/1pHX/88cpms1q4cKFuueWWSZ+F9fT06Nlnn1WtVpvy2O69914tWbJEpVJJ5XJZb3/723XrrbdOmqtUKrrmmmvU0dGhYrGoiy66SPv27Zv0fr72ewoHv+5833336frrr9fs2bNVLBb1vve9Ty+//PKUx+bx9re/XTNmzND27dsn/Nv33nuv/vZv/1adnZ0qFAoaHByUJG3ZskXnn3++WltbVSgUtGzZMj3++OOT9v7nf/6nTjvtNOVyOS1YsEBf+9rXDvnvH+7xuvrqqzVv3jxls1l1dXVp5cqV6u3t1WOPPabTTjtNkrRq1arw5bC77ror/P0jfYy9vb169tlnNTo6OuX53Lhxo84880y1tbWppaVFp5xySrhmX6vZbOrzn/+8urq6lMvl1N3drW3btk2Yef33FHbs2KEoivTlL39ZX/nKVzR37lzl83ktW7ZMP/vZz6Y8NvzmeKXwFtDX16cVK1boT//0T3XppZdq1qxZrr8/OjqqZcuWadeuXfrYxz6mE044Qf/1X/+l6667Tj09PVq3bl2Yve6663T33Xdr+/btb/gNvo0bN+rP/uzP1N3drVtuuUWS9Mwzz+jxxx/XVVddNWH2r/7qr9Te3q7Vq1drx44dWrdunT71qU/pvvvum/LYP//5zyuKIv31X/+19u7dq3Xr1unss8/Wk08+qXw+7zoPh3PgwAEdOHBACxcunPD2m2++WZlMRtdee60qlYoymYx+8IMfaMWKFVqyZIlWr16tRCKh9evXa/ny5frhD3+o008/XZK0detWnXvuuero6NCaNWtUr9e1evVq02M3PDyspUuX6plnntGHP/xh/dEf/ZF6e3v1yCOP6JVXXtGiRYt000036cYbb9RHP/pRLV26VJL0rne9S5KOyjHedtttWrt2rTZt2vSGPxDw85//XBdccIEWL16sm266SdlsVtu2bTtkIP393/+9EomErr32Wg0MDOiLX/yi/uIv/kJbtmyZ8hzdc889Ghoa0ic/+UmNj4/r1ltv1fLly7V161b38wNOMd40n/zkJ+PXn/Jly5bFkuI77rhj0rykePXq1ZPePnfu3Piyyy4Lf7755pvjYrEYP//88xPmPve5z8XJZDLeuXNneNtll10WS4q3b9/+hsd61VVXxeVyOa7X64edWb9+fSwpPvvss+NmsxnefvXVV8fJZDLu7++f8H4uW7Ys/HnTpk2xpLizszMeHBwMb7///vtjSfGtt976hsd3OJLiv/zLv4z37dsX7927N96yZUvc3d0dS4r/4R/+YcK/feKJJ8ajo6Ph7zabzfikk06KzzvvvAnvz+joaDx//vz4nHPOCW+78MIL41wuF7/00kvhbb/4xS/iZDI56TF+/eN14403xpLif/mXf5l0/Af/3SeeeCKWFK9fv37S/z8ax7h69epYUrxp06ZJx/RaX/nKV2JJ8b59+w47c/D8Llq0KK5UKuHtt956aywp3rp1a3jbZZddFs+dOzf8efv27bGkOJ/Px6+88kp4+5YtW2JJ8dVXX/2Gx4ffHF8+egvIZrNatWrVr/33H3jgAS1dulTt7e3q7e0N/5199tlqNBravHlzmL3rrrsUx/GUPwbY1tamkZERbdy4ccp//6Mf/eiEH3FcunSpGo2GXnrppSn/7sqVK1UqlcKfL7nkEs2ZM0ff/e53p/y7h/PNb35THR0dmjlzps444ww9/vjjuuaaa/TpT396wtxll1024dXIk08+qRdeeEF//ud/rr6+vnAeR0ZG1N3drc2bN6vZbKrRaGjDhg268MILdcIJJ4S/v2jRIp133nlTHt+3v/1tveMd79BFF1006f9N9aOiR+sY16xZoziOp/yx4ba2NknSv/7rv075DeJVq1ZN+F7SwVc8L7744hv+PUm68MIL1dnZGf58+umn64wzzviNrgvY8OWjt4DOzs7f6BuxL7zwgp5++ml1dHQc8v/v3bvXvfPKK6/U/fffrxUrVqizs1PnnnuuPvjBD+r888+fNPvaDzqS1N7eLulXX7aZykknnTThz1EUaeHChdqxY4f7mA96//vfr0996lOKokilUkmnnnqqisXipLn58+dP+PMLL7wg6VdhcTgDAwOqVCoaGxubdOySdMopp0z5geuXv/ylLr74Ysu7MsmbdYyH86EPfUh33nmnrrjiCn3uc59Td3e3PvCBD+iSSy6Z9E36I3ldSL/6AYL777//1zpu2BEKbwHer503Go0Jf242mzrnnHP02c9+9pDzJ598svuYZs6cqSeffFIbNmzQ9773PX3ve9/T+vXrtXLlSt19990TZpPJ5CF3xMfoN712dXXp7LPPnnLu9ef94Ge+X/rSlw77Y6AtLS2qVCq/8TH+uo71MebzeW3evFmbNm3Sd77zHT366KO67777tHz5cn3/+9+fcC281a4L2BAKb2Ht7e3q7++f8LZqtaqenp4Jb1uwYIGGh4dNHwg9MpmM3vve9+q9732vms2mrrzySn3ta1/TDTfcMOmbtr+ug5/5HhTHsbZt26bFixcfkf0eCxYskCSVy+U3PJcdHR3K5/OTjl361T0Jln9nqp+kOdyXkd6sY3wjiURC3d3d6u7u1j/+4z/qC1/4gv7mb/5GmzZtOmLX4KGO+/nnn+fu5zcB31N4C1uwYMGE7wdI0te//vVJrxQ++MEP6kc/+pE2bNgwaUd/f7/q9Xr4s/VHUvv6+ib8OZFIhA/UR/Kz0IM/ZXLQgw8+qJ6enmNyk9mSJUu0YMECffnLX9bw8PCk/3/wx2yTyaTOO+88Pfzww9q5c2f4/88888whH4PXu/jii/XUU0/poYcemvT/Dn4WffDLXa//pOBoHaP1R1L3798/6W0HX7Ecyevi4Ycf1q5du8Kff/zjH2vLli3H5Lr4fcMrhbewK664Qh//+Md18cUX65xzztFTTz2lDRs2aMaMGRPmPvOZz+iRRx7RBRdcoMsvv1xLlizRyMiItm7dqgcffFA7duwIf8f6I6lXXHGF9u/fr+XLl6urq0svvfSSvvrVr+qd73znEb0De9q0aTrzzDO1atUqvfrqq1q3bp0WLlyoj3zkI2Hmscce0x//8R9r9erVh7xv40hJJBK68847tWLFCp166qlatWqVOjs7tWvXLm3atEnlcln/9m//Jklau3atHn30US1dulRXXnml6vW6vvrVr+rUU0/V008//Yb/zmc+8xk9+OCD+pM/+RN9+MMf1pIlS7R//3498sgjuuOOO/SOd7xDCxYsUFtbm+644w6VSiUVi0WdccYZmj9//lE5RuuPpN50003avHmz3vOe92ju3Lnau3evbr/9dnV1denMM8/8zR6A11i4cKHOPPNMfeITn1ClUtG6des0ffr0w36JFEcOofAW9pGPfETbt2/XN7/5zfDk3rhxo7q7uyfMFQoF/cd//Ie+8IUv6IEHHtA999yjcrmsk08+WWvXrlVra6v737700kv19a9/Xbfffrv6+/s1e/ZsfehDH9KaNWuO6F2/119/vZ5++mn93d/9nYaGhtTd3a3bb799wh3dBz8jnjNnzhH7dw/nrLPO0o9+9CPdfPPNuu222zQ8PKzZs2frjDPO0Mc+9rEwt3jxYm3YsEHXXHONbrzxRnV1dWnt2rXq6emZMhRaWlr0wx/+UKtXr9ZDDz2ku+++WzNnzlR3d7e6urok/epO37vvvlvXXXedPv7xj6ter2v9+vWaP3/+m3KMh/O+971PO3bs0D/90z+pt7dXM2bM0LJly37t6+xwVq5cqUQioXXr1mnv3r06/fTTddttt70p18Dvuyjmuz44Bg5+9v/AAw/okksuecPZz372s/rWt76lbdu2URnxO27Hjh2aP3++vvSlL+naa6891ofze4nvKeAtb9OmTbrhhhsIBOBNwJeP8Jb3xBNPHOtDAH5v8EoBABDwPQUAQMArBQBAQCgAAALzN5r/5xcn/3KVN5LO239zWKnoLIOrDJpH9+x6xbV6vGb/atq8+ae4dre0lc2zQ8N9Uw+9xt6Xp24kfa1UPPUv2Tmo98C+qYdeo3XGoYv5DmV25x+4dis+dJ/OoWSiMdfqPbu3TT30Gr177L8MKKrbz7ckqWZ/P5vO3aOVqX+RzkGpTM61O5mx/4RYLPv7KEnppO+3tKXT9k6x0jT7NStJY3Fj6qH/L274vkJfmjHbPDto+KVIr3XLjTdMOcMrBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABCYu4/yBd9vvSqU7L+vNYqbrt25tpnm2ZOm+36n6+DQuHk2Stv7nSQpXSiZZ2fkfH1QmXrkmt+ze4d5Nl9oc+0uFaeZZ2tVe4eMJCUdvxbqwMB+1+7I+Tunkil7L1CjXnftTiTsz4ko4XvslbB3DiU8J1xSoWi/xmu1qmv36MiIa35ocNg8Gzt/q19rh/1jUCFv72CSpChlf3yyaWdvnAGvFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACOz3sDvvpC+02G93r4zaqyUk6dVXe8yzsXwVGuW2GebZfKHo2h0l7LekZxy3uktSa5vv/axW7BUDpUbFtTudtdc/pHK+CoBE0v5+lmZ2uHY3q+2u+UIubZ4d6d/r2j3UP2CfHbfXOUhSxvH4NJq+68rzgaLU1uba7H0uVx3XSq3uq9wYHBk0z8axr8qlWq+ZZ/MpX32KBa8UAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQGDuPsrlfR01nn6V/v32HhFJGh+xd71UKkOu3e3lVvNsJvJ1mrS3tphnBxzdN5JUrfmOpVAsm2crtTHX7objUIpFX39Uyl4fpaG+EdfumqMPSpIy2YJ5djjh+/yr7ugcyjqfm/mC/Tqs1J19QzV7F09zzNd55ulTk6TZs9vMs41k1rW7kbZfiKmk77FvxPZzODRwwLXbglcKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgIBQAAAE5pqLUmuba3FTkXm2ffo01+5E3X5rd1L2KgJJymfNp0Tloq9eoOCoI6g6KxfqQ/bjlqRGMzbPjjrrCFocFRrj474KjXQzaZ5NpexVK5KUTqVd84P99rqVVNpXo9DVdYJ5dnjQV+cx7qiiSMhXcxE5Ps9MpX3nu1a3H7ck9fTsM8+WOma7dkcJ+7G35H3XYST7c7PmOA4rXikAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABAQCgCAwFyYk0g6+zsS9rxpxDXX7pZWe1fS0OCAa3cqb+/taaZ958TTOdOIfZ0zhRZfx1O1MmSejRL2HitJKhTt52W07ut4KhZbzbPpQsm1e/+B3a75V3p2mWdrw/bzLUn5lL0ny/foSLW4YZ5NJHydWolUxjybc3SBSVLGsVuSBpP2zq5stsW1u9jabp6Nmt5rvM08Wx21929Z8UoBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAIDAfA/7jOkdrsWjjkqHem3EtbsZ2SsdWgttrt3Zsv329UbKV3PRNzRmnp3mqIqQpPHKqGu+WCqaZ6tNe12AJI2P24+ldbr9fEtSpWKvDIjlq0UYGOh3zTea9ms8X7Sfb0kqJO3zQ8ODrt0jI/bnWyrjuw4T6dg824h9BR1DjYprvs3xXJ4xa45rt9Jp82hctT/vJani+NhZap3p2m3BKwUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQmLuPDvTtcy0ut083z+Zn+HqV+gfsXS+xkq7dLY6Ommwu79rddPQZjQ0ecO32dLFI0v5X+82znr4hSWorTzPPRpH5EpQkpfP29zOd9nUfHZec65pPJBvm2XrV1++Viu2fr+VbfNdhbtDRw+SrJ1IqaT/udN533P3Dvg6uIUfnULLf93xLZrPm2VKLrz+qkLafl0Rs74Ez7zziGwEAv7UIBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACAgFAEBg7hgo5O23dUuSmk3z6NCgrwJAsX20va3VtdpT59HZ2enaXSzab1+Pq0fvln5JKrfbqygadV/NRTKyV1Ekkr56jmKr/fGsN+uu3X0v+6oO9u3tNc8m5DuWuuPxbMmVXbtzBft12Fr27Y5j+5NzzHmNZ3O+ayWTsdd5tJR8dRG5Qot5dmTM9/FtbMRe41PM+qpCLHilAAAICAUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJz91Emm3Mtrtbs3UdxHLl2J1NJ8+zA4LBrdybn6CeKfMc9OmbvekmlzQ+NJCmb9XVTNer2+TjlO5ZEZH98Us7rKrZfVoocHVmSVMz7+m+On2Pvvhoa7HPtHm7Y+6bqnpMiqVC09xkNjo26djcb9mPJ5jKu3SPDQ675uGh/fkbOnqzY8X62tLS7djcb9llvZ5MFrxQAAAGhAAAICAUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAYC61KbdOdy0eHa+ZZ2vDvu6WXKFonm06+29Sjp6f2JmpqYx9vln1nRNPr5IkxbG9n8jTNSVJlYq9R6ZY9nUftba1mWd39+xy7fZ2JbXk7R1C9fGKa3eyZC/AaSTTvt2OnqyMsw8qStivlbHhQdfuUsHeSyZJ+YL92GtV3+NTbLG/n7liq2u3HNdhozHm223AKwUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJzp0PD17qgYov91u5mInLtHh2339pdLLa4dmezGfPs8PCIa3c+Z68XSMp3wtuntbvmFdvvpd+z21cXEUf2x9NTWyFJQ0PD5tl0wlf/0Fr2ncP+vfvMsy+/bJ+VpOnT7NdKsVxy7U5k7TUx9aq9bkPyVZxUa77dWUc9hyQNj9ifn60pX4VGsWQ/56POc5jN2Y+lMl517bbglQIAICAUAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAIzN1HY+M11+JEfdx+EOmca3eybu9XadorfiRJ1Zp9d2dXl2t3vWY/J8MD+127xyq+DpTI0X0UOTuEYtm7Xl7Z1eParaa9EyqTNF/ekqQoYe+9kqTBEcf7ubvftTuTn2GezTZ8F3kxZX8/GzXf8769zd4fNe+Eua7de/bscM2nxu3P5Y5Zna7dQyP2/rVUvuzarcj+ufrIaMW324BXCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAICAUAAABOYegNnH+W4DHxmz3x4/ONTv2t1Sst823td3wLU7k7VXAAyP2m91l6RCPuvYPeLa/erefa75Ur5oni20tLh2Dw0Pm2ejKOnanUrZqytaHdeJJNWqvqqQodEd5tnxmu/9bJt2nHk2iux1G5JUcVy3hZzvHDYbkXl2eNhe+yJJ2aLvWFra7dd4Q77HJ1+0PydyBftxSFLvvlfNs4ViybXbglcKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAIDAXydRqvl6Y9vY282zkjKax6qh5NufoG5KkXC5nno0ie8+LJNWqFfNsNus77kzG3tkk+TqeKuO+x35Gx3T77jH7OZGkwd795tlcKu3avWdvn2v+f9/7bfPsgnnzXbtPmHeSeXZkeI9r94ijmyrh/Lwxm7Y/f0Ycz2NJGhj3zZfbCubZGTNnuXY3ZX/uj437esxKjq6x0RFf/5oFrxQAAAGhAAAICAUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAjMNRcjo746grHRvebZjLOKotasm2dbZ7S6dqsem0eLOd9xpx2tGFG26NqdmX2ca/6F5583z5ZLZdfutKMCIEqZL0FJUq+jKmSfoxJDksot01zzF1zwfvPshkcfde1++JEfmGdXnP8u1+6Zs9vMs8POGoVq0/74DAwPuHYr66stKbfaH89G01dZMz5eM8/OafVVaGRj+8egPTrg2m3BKwUAQEAoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQmItnyq2+DqFE3DTP7nn1Vdfu0bq9j6VR93U2lYsl82w66cvURNPeaZLL5Fy7m3X7+Zak6dM6zLOtZV/3Udkxv2P7DtfuGR1zzLPVqr2fRpIqNd/8iQvnmWcbUcO1++fPvWCeXX72UtfultYW8+yMlumu3fv32/umZnce79o9Vqu65qPI3pWUdMxK0rQ2+8fDbJRx7c407NdhKuU7bgteKQAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAIDA3H1ULBZ9ixP2rpcnfrzdtfsH//7v5tnxsXHX7nPPO888+7bFi1y7R4aGzbOPPPRd1+45s2e55k8+5RTz7BM/edq1e+ZM+7HMmz/ftbunZ495dmzM12XUMWuaa753sN88m8iZn2qSpIGhIfPsWM3eqSVJI/bqMBVbsq7dM2fPNc9Gzk9Ja3Vf99HoqL33LJ3wdQi1FOz9UYN99j4oSUo6OrtqSV/nmQWvFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACMz33o856yKiuG6ebSv5KjTqA4Pm2T0v73LtfmLzZvPsk0/82LV7vGI/J9Vx5+3rjaRvvmm/rX9fb69r9c6X7FUUNV8Thf7PT/+vefa5559z7f7EVZ9wzZend5hnF5zsq0TZuvVn5tk7vnGXa3epYH++rbr8ctfuhQsXmGcz6ci1u9n01Vzk0vb5Ezpnu3bv2LnTPLtzp6/Gp2uW/VhGxn0fly14pQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACc/fRs8+94FpcGR02z470+fqJktVR82xr0tevUojsnUONyohrd2PUXvRTG49du3cPDrjme3vs5zyTzbp2/+K55+3DTXsflCTVHF0vu3a+5Np95/+60zV/8h+eap7NJQuu3aV8q3l2uH+/a3d7sWye3fPKbtfu/Xv7zLO7d9v7gySppeC7Dl/cZr8OPV1tktTX+6p5dnh0yLX7PSsuMs+e+D8Wu3Zb8EoBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAIDAXHMxf8E81+LxYXsFxM7RXtfuoV77rfdDfb4KgBeestdFtLTaqwgkKZWxVx3EzaRrdzblm0/VM/bZqOLa3e5oI6j02+sCJMlTWjK9xXx5S5JSNXuFhiTVD9grHQrOGoWy4x3Nt+Rcu9vy9mvllRefce2uVu1VLvv2+h77jmntrvlXfvmseTatqmv3+PigebZS9e2Oa/bnW91+us14pQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACczlMT88+1+LWor2PpbUl79r9h3843zw72Fdy7R5xdDbte3WHa/fg8Jh5th77uozS6bRrPpGwl+s0XZulWtP+ucbW3pdcu5NJe59RvebrnDkw5uvJGux5zjwbRZ7WJmlw0N6t41UbsHdwvbr96HUfjY/7OrW2R7FrPhnb9xezvt0tefs1XvQ9NRVX7B8nEglH0Zh15xHfCAD4rUUoAAACQgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJzZ0B7e5trcW3MXhcxPDjg2p3J2isDjpvT7tqdS882zzYXnODaPeq4rX+0Yq8LkKTIUVshScmEvUYjk3XeSp+yzzd97QJqNBrm2UrVV6NQzPv6COp1e43G6Ij9+SBJIyNl+3E0fcedztprLrI5e12NJMVNeylK3f5QSpIaDV/hSmX0gHm2nPc9f1QbMo/m0r4an+PnHGeeLZfaXLsteKUAAAgIBQBAQCgAAAJCAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAnP3UWu51bV4zBE3uw4Mu3ZHRXsvjJq+/ptGMmMfdoxKUpyxH0su4SsF8nQCSVIiaX+Axmu+3c3IfFmpkHc8lpJUt/fftKZ9n/PMnDnNNT8yuMc+vM8xK6k4bYZ5NlW2d+VIUspxzufNn+faPTzkeS77Hp+co7NJksZG7MeSiOw9VpK0v/dl8+z0fMm1Ozltlnm2UvX1QVnwSgEAEBAKAICAUAAABIQCACAgFAAAAaEAAAgIBQBAQCgAAAJCAQAQEAoAgMDcRxA776ZOp+wdEPNOWuTaPdrRZp4dGx5w7e7tPWCeLeRbXLvbc/ZzUhkbdO2WfLUY6XTaPLuvt9e1u5nKmWc7Zx7v2j02bK8uGB7ud+3OtPgqN0bG+8yzzaT9fEtSrWGfT0VZ1+5ZHZ3m2dLMLtfuTKu9yqXZjFy702lfzUU5su9vOOtwUu3TzbMzy22u3aNN+2Mf+06hCa8UAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQGDuPqpW677FCXt/R+fxJ7p2j86cbZ8dr7l2z8/Ye2SiyHz6JEm9u3eZZ0f7drt2R5Ev39vbWs2zc0/Ou3aPxfZrpZTz9Ucl6vbHc+eLz7h2z5ph72ySpIG+F82zoxXfdXj8gvnm2dPffb5rd++AvT/K262Tytif9/lCybW7Edu7wySpUbf3gRXyvt6rSqNqnm2dc5xrdzxs72Eaq9iPw4pXCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAICAUAAABIQCACAwl/dkM75emLqjo6bWSLp258sd5tlU2d7FIslV9tJs+lYX2u2dQJHsvS2S1NExwzU/MDBonh2XrwAnN83++NQrvk6trKNvqm3aNNfuoUF7N5UkDQwMmWenzZrj2n3au88yzzaTvk6gQsHeN1UqFV27BwYGzLP7+g64dqfyba75Uot9PpXzncPZJywwzyZzvo+dUcN+XeXTzo9vBrxSAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgMHcGVKq+OoJqzd4BkcrmXbsV2WsX6lV73YYk1ev2405EvlvMy+3t5tlUouLaXYt8tRjNlP0ctk6b7tudsT+e49Vh1+6ko/4j4TgOSTrgqGiQJMelovnz/sC1O9tir+hoKOvaXSiWzbP1hu95X6vbH58Zs2a5dudbZ7rmGw3757zNZsO329FxU3NWuWQK9scnJd/HNwteKQAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAIDA3H3UjH3dIOmMebWacpTISKpUx+zDzu6WdGQ/7mTSd9z1etU+nPLlte/RkeqODiElfcfi6ZEZHXM8lpJyxZx5Npktunanci2u+VnHLXDMnuja3dc/ap4tljKu3U3HcyKfK7h2F0v23p6q7+nj/jihKGkeTaV95zAZ27vDPD1JkhTLvlvy9a9Z8EoBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEBAKAIDA3OmQSvvyo96w1yhUGzXf7th+m34yZb/VXZI8d6RX677jTjhuu6/XPbe6S+lM1jXf2pY3zzrv0lck++OTSdtrRSQpkbTXEZTap7t218ZPcM3v2rnbPBslfRUaLaVp5tnBoSHX7rSjtiR2tKFIUmW8Yp6Nsr5qiWpl3DWfTtqfb7GjtkKSosh+YtKRb3dD9uOOjsLn9bxSAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgIBQAAIG5eGZkZNC1uNZMm2cTaV83SJywd4OMOzqYJCny9JQkfMfdbNozOJu1d99IcpfUNBoN82zK0SEjSdXqfvNsa8nXCZRK2juems7Hvqaca376zLnm2WKL7/FsOI69xXkOY8djHzl6kiQp5egzilK+3qt6w96pJUkNR0eap8tIkhKRvRAs7fw4UXO8n85aMhNeKQAAAkIBABAQCgCAgFAAAASEAgAgIBQAAAGhAAAICAUAQEAoAAACQgEAEJjvMy+VWl2Lxyv2W7XHq1XX7mzGXqGh2H5LvyTV6vYbx2P5bl9PxPb5KOnc7byVvlG1n5da03czfTZbdByIr1qimrTXKGQ8lSWS5sw91TWfzdqPvV7zXYdDQwPm2WTC97ldLm9//oyPj7l2R5G9uqLZ9NVcpNOO572khOM5FDsrUTyfTzdi3/OnXrd/PEw4H3vTziO+EQDwW4tQAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABAQCgCAgFAAAASEAgAgiOI49pZ+AAB+R/FKAQAQEAoAgIBQAAAEhAIAICAUAAABoQAACAgFAEBAKAAAAkIBABD8P1LQY4e9qNhAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}